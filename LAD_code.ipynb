{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAD Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI \n",
    "import pandas as pd\n",
    "import base64\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from enum import Enum\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage I: Perception\n",
    "* Image description\n",
    "* Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1_1_zh = '''\n",
    "请结合中文文化背景，用一段话描述图片。你应该考虑图片中的角色、文字、颜色和布局。重点关注文字和图片中的重要元素。尽量简洁，同时确保描述的准确性。\n",
    "'''\n",
    "\n",
    "prompt1_1_en = '''\n",
    "Please provide a description of the image in a paragraph. You should consider the role, text, color, and layout of the image. Focus on the text and important elements in the image. Try to be concise while ensuring the correctness of the description.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1_2_zh = '''\n",
    "请结合中文文化语境，根据图片描述提供七个与隐喻最相关的关键词。你应该考虑图片描述中的情感、领域和修辞手法。重点关注图片描述中的图片文字和重要实体。请注意，可能存在谐音梗和双关语，请识别并提供它们，但不要重复相同的元素。深呼吸并一步步思考，仅输出关键词。\n",
    "图片描述：{}\n",
    "'''\n",
    "\n",
    "prompt1_2_en = '''\n",
    "Please provide seven keywords most related to the metaphor based on the image description. You should consider the emotion, domain, and rhetoric in the image description. Focus on the image’s text and important entities in the image description. Note that there may be homophonic memes and puns, distinguish and provide them but do not repeat the same element. Take a deep breath and think step by step, only output the keywords.\n",
    "Image description: {}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT_p1_1(prompt1,url):\n",
    "    base64_image = encode_image(url)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt1},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\", \"detail\": \"high\"}}\n",
    "        ]}\n",
    "    ]\n",
    "    \n",
    "    api_key = \"\"\n",
    "    proxy_api_url = ''\n",
    "    headers = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {api_key}', }\n",
    "    data = {'model': 'gpt-4o-mini', 'messages': messages, 'temperature': 0.7, 'top_p': 0.9}\n",
    "    response = requests.post(proxy_api_url, headers=headers, json=data)\n",
    "    image_dep = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return image_dep\n",
    "\n",
    "def GPT_p1_2(prompt2,image_dep):\n",
    "    messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt2.format(image_dep)}\n",
    "        ]\n",
    "\n",
    "    api_key = \"\"\n",
    "    proxy_api_url = ''\n",
    "    headers = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {api_key}', }\n",
    "    data = {'model': 'gpt-4o-mini', 'messages': messages, 'temperature': 0.7, 'top_p': 0.9}\n",
    "    response = requests.post(proxy_api_url, headers=headers, json=data)\n",
    "    keywords = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage II：Search\n",
    "* Construct search questions\n",
    "* Self-judge\n",
    "* ModelSearch\n",
    "* WebSearch\n",
    "* Rank\n",
    "* Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2_1_zh = '''\n",
    "# Role\n",
    "你是一位熟悉网络文化和梗的研究员，擅长挖掘和解析网络梗的深层含义。\n",
    "## Attention\n",
    "你负责基于关键词，设计出能够精确检索到梗文化或图片隐喻的检索问题。\n",
    "\n",
    "## Skills\n",
    "### Skill 1: 网络搜索技能\n",
    "- 能够分析提供的关键词并选择最相关的关键词，理解其在隐喻文化中可能的含义和应用\n",
    "\n",
    "### Skill 2: 文化分析\n",
    "- 能够深入挖掘关键词在网络文化中的背景和历史，通过信息整理和分析，找出关键词背后的深层含义\n",
    "\n",
    "### Skill 3: 隐喻理解\n",
    "- 能够理解和分析关键词在图片中的隐喻意义，更好地理解梗图的文化背景\n",
    "\n",
    "## Workflow:\n",
    "1. 分析提供的关键词并选择最相关的关键词，理解其在隐喻文化中可能的含义和应用。\n",
    "2. 设计具体且有针对性的检索问题，以提高搜索的精确度。\n",
    "3. 确保检索问题能够引导用户找到与梗图相关的文化背景或隐喻解释。\n",
    "\n",
    "## Constraints\n",
    "- 检索问题需要围绕梗文化或图片隐喻，避免过于广泛或不相关的搜索结果\n",
    "- 检索问题需要考虑关键词的组合，尽量避免单一关键词检索\n",
    "- 检索问题总数为5个，请设计最相关的问题\n",
    "\n",
    "## Example: \n",
    "输入：\n",
    "\"\"\"\n",
    "关键词：讽刺，环保，浪费，对比，空喊口号，行动缺失，言行不一致\n",
    "\"\"\"\n",
    "输出：\n",
    "\"\"\"\n",
    "1. \"环保口号与实际行动不符的具体表现？\"\n",
    "\n",
    "2. \"网络上流行的环保与浪费对比梗图的深刻内涵？\"\n",
    "\n",
    "3. \"讽刺环保口号与行动缺失的双重信息图片？\"\n",
    "\n",
    "4. \"环保主题中口号与实际行为矛盾的讽刺性对比图？\"\n",
    "\n",
    "5. \"网络梗图如何展现环保口号与浪费行为的讽刺对比？\"\n",
    "\"\"\"\n",
    "\n",
    "## Solve:\n",
    "关键词：{}\n",
    "'''\n",
    "\n",
    "prompt2_1_en = '''\n",
    "# Role\n",
    "You are a researcher familiar with internet culture and memes, skilled at uncovering and analyzing the deeper meanings of internet memes.\n",
    "## Attention\n",
    "Your responsibility is to design precise search questions based on keywords that can accurately retrieve meme culture or image metaphors.\n",
    "\n",
    "## Skills\n",
    "### Skill 1: Internet Search Skills\n",
    "- Analyze the provided keywords and select the most relevant ones, understanding their potential meanings and applications in metaphorical culture\n",
    "\n",
    "### Skill 2: Cultural Analysis\n",
    "- Deeply explore the background and history of keywords in internet culture, through information organization and analysis, to uncover the deeper meanings behind the keywords\n",
    "\n",
    "### Skill 3: Metaphor Comprehension\n",
    "- Understand and analyze the metaphorical meanings of keywords in images, to better comprehend the cultural background of meme images\n",
    "\n",
    "## Workflow:\n",
    "1. Analyze the provided keywords and select the most relevant ones, understanding their potential meanings and applications in metaphorical culture.\n",
    "2. Design specific and targeted search questions to improve the accuracy of the search.\n",
    "3. Ensure that the search questions can guide users to find cultural backgrounds or metaphorical explanations related to the meme images.\n",
    "\n",
    "## Constraints\n",
    "- Search questions should focus on meme culture or image metaphors, avoiding overly broad or irrelevant search results\n",
    "- Search questions should consider keyword combinations, avoiding single keyword searches as much as possible\n",
    "- The total number of search questions should be 5, please design the most relevant questions\n",
    "\n",
    "## Example:\n",
    "Input:\n",
    "\"\"\"\n",
    "Keywords: irony, environmental protection, waste, contrast, empty slogans, lack of action, inconsistency between words and deeds\n",
    "\"\"\"\n",
    "Output:\n",
    "\"\"\"\n",
    "1. \"What are the specific manifestations of the inconsistency between environmental protection slogans and actual actions? \"\n",
    "\n",
    "2. \"What is the profound connotation of the popular Internet pictures comparing environmental protection and waste? \"\n",
    "\n",
    "3. \"Dual information pictures satirizing environmental protection slogans and lack of action? \"\n",
    "\n",
    "4. \"Ironic contrast pictures of the contradiction between slogans and actual actions in environmental protection themes? \"\n",
    "\n",
    "5. \"How do Internet pictures show the ironic contrast between environmental protection slogans and wasteful behavior? \"\n",
    "\"\"\"\n",
    "\n",
    "## Solve:\n",
    "keywords: {}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2_2_zh = '''\n",
    "# Task\n",
    "请自我评估知识水平，判断问题是否适合直接回答或需要外部知识支持，并根据评分准则提供置信度分数。\n",
    "\n",
    "## Evaluation Standard\n",
    "- 若问题中包含网络文化和梗文化，则置信度分数小于3\n",
    "- 若问题中包含的知识流行度较高，则置信度分数小于3\n",
    "- 若问题中包含的实时性内容占比多，则置信度分数小于3\n",
    "- 若问题中包含的实体相对小众，则置信度分数小于3\n",
    "- 若问题中包含的实体多于2个，则置信度分数小于3\n",
    "\n",
    "## Workflow:\n",
    "  1. <隐式>分析问题内容，识别是否包含网络文化和梗文化元素。\n",
    "  2. <隐式>评估问题的流行度和实时性内容占比。\n",
    "  3. 根据评分准则，<显示>提供置信度分数和决策。\n",
    "  \n",
    "## Constraints\n",
    "- 置信度分数必须在1-5分，分数大于3时选择ModelSearch，分数小于等于3时选择WebSearch\n",
    "\n",
    "## OutputFormat: \n",
    "[置信度分数(1-5分), 决策(ModelSearch/WebSearch)]\n",
    "\n",
    "## Solve:\n",
    "问题：{}\n",
    "'''\n",
    "\n",
    "prompt2_2_en = '''\n",
    "# Task\n",
    "Please evaluate your knowledge level, determine whether the question is suitable for direct answering or requires external knowledge support, and provide a confidence score according to the Evaluation Standard.\n",
    "\n",
    "## Evaluation Standard\n",
    "- If the question contains Internet culture and meme culture, the confidence score is less than 3\n",
    "- If the knowledge contained in the question is highly popular, the confidence score is less than 3\n",
    "- If the real-time content contained in the question accounts for a large proportion, the confidence score is less than 3\n",
    "- If the entity contained in the question is relatively niche, the confidence score is less than 3\n",
    "- If the question contains more than 2 entities, the confidence score is less than 3\n",
    "\n",
    "## Workflow:\n",
    "1. <Implicitly> Analyze the content of the question and identify whether it contains Internet culture and meme culture elements.\n",
    "2. <Implicitly> Evaluate the popularity of the question and the proportion of real-time content.\n",
    "3. According to the scoring criteria, <Explicitly> provide a confidence score and decision.\n",
    "\n",
    "## Constraints\n",
    "- The confidence score must be between 1 and 5. If the score is bigger than 3, select ModelSearch. If the score is smaller than or equal to 3, select WebSearch.\n",
    "\n",
    "## OutputFormat:\n",
    "[Confidence score (1-5 points), Decision (ModelSearch/WebSearch)]\n",
    "\n",
    "## Solve:\n",
    "Question: {}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2_3_zh = '''\n",
    "# Role\n",
    "你是一位熟悉网络文化和梗的研究员，擅长挖掘和解析网络梗的深层含义。\n",
    "## Attention\n",
    "你负责基于网络文化和梗文化的问题，提供关于网络梗和图片隐喻的清晰、专业、全面的解释，帮助用户理解其背后的文化含义和社会影响。\n",
    "\n",
    "## Skills\n",
    "### Skill 1: 隐喻理解\n",
    "- 能够根据图片分析准确识别图片中涉及的梗或隐喻，并能提取深层含义\n",
    "\n",
    "### Skill 2: 网络文化分析\n",
    "- 能够识别经典影视角色、经典影视桥段并分析梗文化或图片隐喻的起源、发展和当前流行状态，让用户更好地理解梗图的文化背景\n",
    "\n",
    "### Skill 3: 提供解释\n",
    "- 能够为用户提供清晰、专业的解释，解释梗或图片隐喻背后的文化含义和社会影响\n",
    "- 能够提供结构化的、易于理解的回答\n",
    "\n",
    "## Workflow:\n",
    "1. 确定用户提出的问题中涉及的梗或图片隐喻。\n",
    "2. 分析梗或图片隐喻的起源、发展和当前流行状态。\n",
    "3. 解释梗或图片隐喻背后的文化含义和社会影响。\n",
    "4. 提供结构化的、易于理解的回答。\n",
    "\n",
    "## Constraints\n",
    "- 回答内容需要逻辑清晰，层次分明，确保读者易于理解\n",
    "- 回答内容需要围绕梗文化或图片隐喻\n",
    "- 回答部分需要全面且完备，不要出现\"基于上述内容\"等模糊表达，确保信息的可信度\n",
    "- 语言风格需要专业、严谨，避免口语化表达\n",
    "- 保持统一的语法和词汇使用，确保整体文档的一致性和连贯性\n",
    "\n",
    "## Solve:\n",
    "问题：{}\n",
    "'''\n",
    "\n",
    "prompt2_3_en = '''\n",
    "# Role\n",
    "You are a researcher familiar with internet culture and memes, skilled at uncovering and analyzing the deeper meanings of internet memes.\n",
    "## Attention\n",
    "You are responsible for providing clear, professional and comprehensive explanations of Internet memes and image metaphors based on Internet culture and meme culture, helping users understand the cultural meaning and social impact behind them.\n",
    "\n",
    "## Skills\n",
    "### Skill 1: Metaphor Understanding\n",
    "- Ability to accurately identify the memes or metaphors involved in the picture based on picture analysis, and be able to extract the deep meaning\n",
    "\n",
    "### Skill 2: Internet Culture Analysis\n",
    "- Ability to analyze the origin, development and current popularity of memes or image metaphors, so that users can better understand the cultural background of memes\n",
    "\n",
    "### Skill 3: Providing Explanations\n",
    "- Ability to provide users with clear and professional explanations of the cultural meaning and social impact behind memes or image metaphors\n",
    "- Ability to provide structured and easy-to-understand answers\n",
    "\n",
    "## Workflow:\n",
    "1. Identify the meme or picture metaphor involved in the question raised by the user.\n",
    "2. Analyze the origin, development and current popularity of the meme or picture metaphor.\n",
    "3. Explain the cultural meaning and social impact behind the meme or picture metaphor.\n",
    "4. Provide a structured and easy-to-understand answer.\n",
    "\n",
    "## Constraints\n",
    "- The answer content needs to be logically clear and well-structured to ensure that readers can understand it easily\n",
    "- The answer content needs to focus on the meme culture or picture metaphor\n",
    "- The answer part needs to be comprehensive and complete, and there should be no vague expressions such as \"based on the above content\" to ensure the credibility of the information\n",
    "- The language style needs to be professional and rigorous, and avoid colloquial expressions\n",
    "- Maintain uniform grammar and vocabulary to ensure the consistency and coherence of the overall document\n",
    "\n",
    "## Solve:\n",
    "Question: {}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2_4_zh = '''\n",
    "# Role\n",
    "你是一位熟悉网络文化和梗的研究员，擅长从图像中提取深层含义，并将这些含义与相关的问题和总结进行匹配和排序。\n",
    "## Attention\n",
    "你负责根据图片描述，对多个问题-总结对进行相关性排序，并选择最相关的3个，帮助用户理解图片的深层含义。\n",
    "\n",
    "## Skills\n",
    "### Skill 1: 图片分析\n",
    "- 能够根据用户对图片的描述进行全面分析\n",
    "\n",
    "### Skill 2: 隐喻理解\n",
    "- 能够根据图片分析准确识别图片中的隐喻，并能提取深层含义\n",
    "\n",
    "### Skill 3: 相关性排序\n",
    "- 能够根据图片的隐喻理解，针对问题-总结对的内容是否与图片隐喻相关，对问题-总结对进行相关性排序\n",
    "- 能够提供对应相关性排序理由，并分析该问题-总结对中低相关内容\n",
    "\n",
    "## Workflow:\n",
    "  1. 仔细分析图片描述，<隐式>提取图片中的隐喻和关键元素。\n",
    "  2. 对每个问题-总结对进行分析，<隐式>评估其与图片隐喻的相关性。\n",
    "  3. 根据相关性大小，<隐式>对问题-总结对进行排序。\n",
    "  4. <显示>输出排序后最相关的3个问题-总结对和排序理由。\n",
    "\n",
    "## Constraints\n",
    "- 排序应基于图片隐喻的相关性，确保排序结果的准确性和逻辑性\n",
    "- 若问题-总结对中某部分与图片隐喻无关，请在对应理由中提供\n",
    "- 仅输出排序后的最相关的3个问题-总结对和排序理由，避免使用markdown格式\n",
    "\n",
    "## Examples:\n",
    "输入：\n",
    "\"\"\"\n",
    "图片描述：\"一个人站在分岔路口\"，问题-总结对：\"1. 问题：\"人生选择的重要性\"; 2. 问题：\"面对困难时的决策过程\"; 3. 问题：\"城市规划的复杂性\"; 4. 问题：\"旅行中的导航技巧\"; 5. 问题：\"个人成长与自我发现\"\"\n",
    "\"\"\"\n",
    "输出：\n",
    "\"\"\"\n",
    "1. 问题：人生选择的重要性\n",
    "  - 排序理由：xxx\n",
    "2. 问题：面对困难时的决策过程\n",
    "  - 排序理由：xxx\n",
    "3. 问题：个人成长与自我发现\n",
    "  - 排序理由：xxx\n",
    "\"\"\"\n",
    "\n",
    "## Solve:\n",
    "图片描述：{}\n",
    "问题-总结对：{}\n",
    "'''\n",
    "\n",
    "prompt2_4_en = '''\n",
    "# Role\n",
    "You are a researcher familiar with Internet culture and memes, and are good at extracting deep meanings from images, and matching and sorting these meanings with relevant questions and summaries.\n",
    "## Attention\n",
    "You are responsible for sorting the relevance of multiple question-summary pairs based on the image description, and selecting the three most relevant ones to help users understand the deep meaning of the image.\n",
    "\n",
    "## Skills\n",
    "### Skill 1: Image Analysis\n",
    "- Ability to conduct a comprehensive analysis based on the user's description of the image\n",
    "\n",
    "### Skill 2: Metaphor Understanding\n",
    "- Ability to accurately identify metaphors in images based on image analysis and extract deep meaning\n",
    "\n",
    "### Skill 3: Relevance Sorting\n",
    "- Ability to sort the relevance of question-summary pairs based on the metaphorical understanding of the image, based on whether the content of the question-summary pair is related to the image metaphor\n",
    "- Ability to provide reasons for the corresponding relevance sorting, and analyze the low-relevance content of the question-summary pair\n",
    "\n",
    "## Workflow:\n",
    "1. Carefully analyze the image description and <implicitly> extract the metaphors and key elements in the image.\n",
    "2. Analyze each question-summary pair and <implicitly> evaluate its relevance to the image metaphor.\n",
    "3. According to the relevance, <implicitly> sort the question-summary pairs.\n",
    "4. <display> Output the top 3 most relevant question-summary pairs after sorting and the reasons for sorting.\n",
    "\n",
    "## Constraints\n",
    "- Sorting should be based on the relevance of the image metaphor to ensure the accuracy and logic of the sorting results\n",
    "- If a part of the question-summary pair is not related to the image metaphor, please provide it in the corresponding reason\n",
    "- Only output the top 3 most relevant question-summary pairs after sorting and the reasons for sorting, avoid using markdown format\n",
    "\n",
    "## Examples:\n",
    "Input:\n",
    "\"\"\"\n",
    "Image Description: \"A person standing at a crossroads\", Question-Summary Pair: \"1. Question: \"The importance of life choices\"; 2. Question: \"Decision-making process in the face of difficulties\"; 3. Question: \"Complexity of urban planning\"; 4. Question: \"Navigation skills in travel\"; 5. Question: \"Personal growth and self-discovery\"\"\n",
    "\"\"\"\n",
    "Output:\n",
    "\"\"\"\n",
    "1. Question: The importance of life choices\n",
    "  - Sorting Reason: xxx\n",
    "2. Question: Decision-making process in the face of difficulties\n",
    "  - Sorting Reason: xxx\n",
    "3. Question: Personal growth and self-discovery\n",
    "  - Sorting Reason: xxx\n",
    "\"\"\"\n",
    "\n",
    "## Solve:\n",
    "Image Description: {}\n",
    "Question-Summary Pair: {}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2_5_zh = '''\n",
    "# Role\n",
    "你是一位熟悉网络文化和梗的研究员，擅长根据相关性排序理由，对选定的“问题-总结对”进行改写，以提高内容的吸引力和信息的准确性。\n",
    "## Attention\n",
    "你负责根据相关性排序理由，对选定的“问题-总结对”进行改写，去掉重复语意、与网络文化和梗文化相关度低的部分，以及去除markdown和引用格式符，最终输出一篇融合并改写后的内容。\n",
    "\n",
    "## Skills \n",
    "### Skill 1: 网络文化洞察\n",
    "- 能够深入理解经典影视角色、经典影视桥段和梗文化，准确把握网络热点和流行趋势\n",
    "\n",
    "### Skill 2: 内容分析与改写\n",
    "- 能够对“问题-总结对”进行细致分析，识别与网络文化和梗文化的相关性\n",
    "- 能够根据分析结果，对内容进行创造性改写，提高信息的准确性和吸引力\n",
    "\n",
    "### Skill 3: 格式识别与处理\n",
    "- 能够识别并去除markdown和引用格式符，确保输出内容的简洁性和易读性\n",
    "- 能够整合改写后的内容，用适当的过渡语句连接，形成一篇连贯、流畅的文章\n",
    "\n",
    "## Workflow:\n",
    "1. 阅读并理解用户提供的相关性排序理由和选定的“问题-总结对”。\n",
    "2. 根据排序理由，分析每个“问题-总结对”与网络文化和梗文化的关联度。\n",
    "3. 对选定的3个“问题-总结对”进行改写，去掉重复语意、相关度低的部分，以及去除格式符。\n",
    "4. 将改写后的内容进行整合，用适当的过渡语句连接，形成一篇连贯、流畅的文章。\n",
    "  \n",
    "## Constraints\n",
    "- 改写后的内容应保持原意，不得改变原有的信息和观点\n",
    "- 语言应简洁明了且易于理解，与图片隐喻相关\n",
    "- 不得使用任何形式的markdown和引用格式符\n",
    "\n",
    "## Solve:\n",
    "相关性排序结果：{}\n",
    "最相关的问题-总结对：{}\n",
    "'''\n",
    "\n",
    "prompt2_5_en = '''\n",
    "# Role\n",
    "You are a researcher familiar with Internet culture and memes, and are good at rewriting the selected \"question-summary pairs\" according to the relevance sorting reasons to improve the appeal of the content and the accuracy of the information.\n",
    "## Attention\n",
    "You are responsible for rewriting the selected \"question-summary pairs\" according to the relevance ranking reasons, removing repeated semantics, parts with low relevance to Internet culture and meme culture, and removing markdown and citation formatting symbols, and finally outputting a fused and rewritten content.\n",
    "## Skills\n",
    "### Skill 1: Internet Culture Insight\n",
    "- Able to deeply understand classic film and television characters, classic film and television plots and meme culture, and accurately grasp network hot spots and popular trends\n",
    "\n",
    "### Skill 2: Content Analysis and Rewriting\n",
    "- Able to conduct detailed analysis of \"question-summary pairs\" and identify the relevance to Internet culture and meme culture\n",
    "- Able to creatively rewrite content based on analysis results to improve the accuracy and attractiveness of information\n",
    "\n",
    "### Skill 3: Format Recognition and Processing\n",
    "- Able to identify and remove markdown and reference format symbols to ensure the simplicity and readability of output content\n",
    "- Able to integrate rewritten content and connect it with appropriate transition sentences to form a coherent and fluent article\n",
    "\n",
    "## Workflow:\n",
    "1. Read and understand the relevance sorting results and all \"question-summary pairs\" provided by the user.\n",
    "2. Based on the selection reasons, analyze the relevance of each \"question-summary pair\" to Internet culture and meme culture.\n",
    "3. Rewrite the selected 3 \"question-summary pairs\", remove repeated semantics, low relevance parts, and format symbols.\n",
    "4. Integrate the rewritten content, connect it with appropriate transition sentences to form a coherent and fluent article.\n",
    "\n",
    "## Constraints\n",
    "- The rewritten content should maintain the original meaning and should not change the original information and viewpoints\n",
    "- The language should be concise, clear, easy to understand, and related to the image metaphor\n",
    "- Do not use any form of markdown and reference format symbols\n",
    "\n",
    "## Solve:\n",
    "Relevance Sorting Results: {}\n",
    "Most Relevant Question-Summary Pair: {}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SearchQuestions\n",
    "def GPT_p2_1(prompt3,keywords):\n",
    "    messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt3.format(keywords)}\n",
    "        ]\n",
    "\n",
    "    api_key = \"\"\n",
    "    proxy_api_url = ''\n",
    "    headers = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {api_key}', }\n",
    "    data = {'model': 'gpt-4o-mini', 'messages': messages, 'temperature': 0.7, 'top_p': 0.9}\n",
    "    response = requests.post(proxy_api_url, headers=headers, json=data)\n",
    "    search_questions = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return search_questions  \n",
    "\n",
    "# Self-Judge\n",
    "def GPT_P2_2(prompt4,search_question):    \n",
    "    messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt4.format(search_question)}\n",
    "        ]\n",
    "\n",
    "    api_key = \"\"\n",
    "    proxy_api_url = ''\n",
    "    headers = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {api_key}', }\n",
    "    data = {'model': 'gpt-4o-mini', 'messages': messages, 'temperature': 0.0, 'top_p': 0.9}\n",
    "    response = requests.post(proxy_api_url, headers=headers, json=data)\n",
    "    score = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return score   \n",
    "\n",
    "# ModelSearch\n",
    "def GPT_P2_3(prompt5,search_question):    \n",
    "    messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt5.format(search_question)}\n",
    "        ]\n",
    "\n",
    "    api_key = \"\"\n",
    "    proxy_api_url = ''\n",
    "    headers = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {api_key}', }\n",
    "    data = {'model': 'gpt-4o-mini', 'messages': messages, 'temperature': 0.5, 'top_p': 0.9}\n",
    "    response = requests.post(proxy_api_url, headers=headers, json=data)\n",
    "    search_result = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return search_result  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WebSearch: GPTAPI\n",
    "class AgentStatusCode(Enum):\n",
    "    STREAM_ING = \"STREAM_ING\"\n",
    "    ANSWER_ING = \"ANSWER_ING\"\n",
    "    PLUGIN_START = \"PLUGIN_START\"\n",
    "    PLUGIN_END = \"PLUGIN_END\"\n",
    "    PLUGIN_RETURN = \"PLUGIN_RETURN\"\n",
    "    END = \"END\"\n",
    "\n",
    "def streaming(raw_response):\n",
    "    for chunk in raw_response.iter_lines(chunk_size=8192, decode_unicode=False, delimiter=b'\\n'):\n",
    "        if chunk:\n",
    "            decoded = chunk.decode('utf-8')\n",
    "            if decoded == '\\r':\n",
    "                continue\n",
    "            if decoded[:6] == 'data: ':\n",
    "                decoded = decoded[6:]\n",
    "            elif decoded.startswith(': ping - '):\n",
    "                continue\n",
    "            response = json.loads(decoded)\n",
    "            yield (response['response'], response['current_node'])\n",
    "\n",
    "def process_response(agent_return, node_name):\n",
    "    if not node_name or node_name in ['root', 'response']:\n",
    "        result =  {\n",
    "            'type': 'planner',\n",
    "            'output': agent_return['response']\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        result = {\n",
    "            'type': 'searcher',\n",
    "            'name': node_name,\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def call_websearch(inputs, server_url='http://127.0.0.1:8002/solve', retries=3):\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        'inputs': inputs,\n",
    "        'agent_cfg': {}\n",
    "    }\n",
    "\n",
    "    attempt = 0\n",
    "    result = {\n",
    "        'planner_output': '',\n",
    "        'searcher_outputs': []\n",
    "    }\n",
    "\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            raw_response = requests.post(server_url, headers=headers, json=data, stream=True)\n",
    "            raw_response.raise_for_status()\n",
    "\n",
    "            current_searcher_output = None\n",
    "            for resp in streaming(raw_response):\n",
    "                agent_return, node_name = resp\n",
    "                processed_result = process_response(agent_return, node_name)\n",
    "                if processed_result['type'] == 'planner':\n",
    "                    result['planner_output'] = processed_result['output']\n",
    "                else:\n",
    "                    if processed_result['type'] == 'searcher':\n",
    "                        current_searcher_output = processed_result['output']\n",
    "                        result['searcher_outputs'].append(current_searcher_output)\n",
    "                        current_searcher_output = None\n",
    "\n",
    "            return result\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"调用 websearch API 时发生错误: {e}\")\n",
    "            attempt += 1\n",
    "            print(f\"第{attempt}次尝试重连...\")\n",
    "            if attempt == retries:\n",
    "                print(f\"已尝试3次重连，自动保存部分结果到文件\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT_P2_4(prompt6,image_dep,search_result_all):    \n",
    "    messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt6.format(image_dep, search_result_all)}\n",
    "        ]\n",
    "\n",
    "    api_key = \"\"\n",
    "    proxy_api_url = ''\n",
    "    headers = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {api_key}', }\n",
    "    data = {'model': 'gpt-4o-mini', 'messages': messages, 'temperature': 0, 'top_p': 0.9}\n",
    "    response = requests.post(proxy_api_url, headers=headers, json=data)\n",
    "    rank_search_result = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return rank_search_result\n",
    "\n",
    "def GPT_P2_5(prompt7,rank_search_result,search_result_all):    \n",
    "    messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt7.format(rank_search_result, search_result_all)}\n",
    "        ]\n",
    "    api_key = \"\"\n",
    "    proxy_api_url = ''\n",
    "    headers = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {api_key}', }\n",
    "    data = {'model': 'gpt-4o-mini', 'messages': messages, 'temperature': 0.5, 'top_p': 0.9}\n",
    "    response = requests.post(proxy_api_url, headers=headers, json=data)\n",
    "    overall_summary = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return overall_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拆分检索问题\n",
    "def split_search_questions(search_questions):\n",
    "    # 按换行符拆分成单独的行\n",
    "    questions_lines = [line for line in search_questions.split('\\n') if line.strip()]\n",
    "\n",
    "    search_question_list = []\n",
    "    # 遍历每行，去除编号和多余的引号，并添加到 search_question_list 中\n",
    "    for line in questions_lines:\n",
    "        # 去除编号和多余的引号\n",
    "        parts = line.split('. ', 1)\n",
    "        if parts != ['']:\n",
    "            if len(parts) > 1:\n",
    "                question = parts[1].replace('\"\"', '\"').strip()\n",
    "            else:\n",
    "                question = parts[0].replace('\"\"', '\"').strip()\n",
    "            # 添加到 search_question_list 中\n",
    "            search_question_list.append(question) \n",
    "    return search_question_list  # 返回拆分后的检索问题列表,还需拆分为单个问题      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取得分，选择search策略(ModelSearch / WebSearch)  \n",
    "def extract_score(score): \n",
    "    match = re.search(r'\\[(\\d),\\s*(WebSearch|ModelSearch)\\]', score)  \n",
    "    score_value = 0  \n",
    "    decision = 'ModelSearch'\n",
    "    if match:\n",
    "        score_value = int(match.group(1))   # 置信度分数\n",
    "        decision = match.group(2)   # 决策\n",
    "    return score_value, decision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WebSearch(search_question):\n",
    "    attempt = 0\n",
    "    while attempt < 3:\n",
    "        print(\"检索中...预计1-2分钟，请稍等...\")\n",
    "        result_all = call_websearch(search_question)\n",
    "        result = result_all['planner_output']\n",
    "        if \"```python\" in result:\n",
    "            attempt += 1\n",
    "            print(f\"识别到内容错误，尝试第 {attempt} 次重新请求...\")\n",
    "        else:\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择搜索策略，调用WebSearch或ModelSearch\n",
    "def choose_search_strategy(score_value, decision, prompt2_3_zh, search_question, i):\n",
    "    if score_value <= 3 and decision == \"WebSearch\":\n",
    "        print(f\"第{i}个问题的置信度分数为：{score_value}，决策为：{decision}，需要进行WebSearch\")\n",
    "        search_result = WebSearch(search_question)\n",
    "    elif score_value > 3 and decision == \"ModelSearch\":\n",
    "        print(f\"第{i}个问题的置信度分数为：{score_value}，决策为：{decision}，需要进行ModelSearch\")\n",
    "        search_result = GPT_P2_3(prompt2_3_zh, search_question)  # 调取ModelSearch\n",
    "    else:\n",
    "        print(f\"第{i}个问题的置信度分数为：{score_value}，决策为：{decision}，决策不符合预期\")\n",
    "        # 默认为ModelSearch\n",
    "        search_result = GPT_P2_3(prompt2_3_zh, search_question)  # 调取ModelSearch\n",
    "    return search_result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_result_top_zh(rank_search_result, search_result_all):\n",
    "    def extract_top_questions(text):\n",
    "        # 提取问题文本,去除编号和排序理由\n",
    "        pattern = r'问题：(.*?)(?=\\n|$)'\n",
    "        matches = re.findall(pattern, text)\n",
    "        questions = [re.sub(r'\\*\\*', '', m).strip() for m in matches]\n",
    "        return questions[:3]  # 只返回前3个问题\n",
    "    \n",
    "    def find_answer(question, all_text):\n",
    "        # 精确匹配问题和答案\n",
    "        # 将问题中的特殊字符转义\n",
    "        escaped_q = re.escape(question)\n",
    "        pattern = f'问题[0-9]?：\"{escaped_q}\"\\\\n回答：\"\"\"(.*?)\"\"\"'\n",
    "        match = re.search(pattern, all_text, re.DOTALL)\n",
    "        return match.group(1).strip() if match else ''\n",
    "    \n",
    "    result_list = []\n",
    "    # 获取前3个问题\n",
    "    questions = extract_top_questions(rank_search_result)\n",
    "    \n",
    "    # 匹配答案并格式化输出\n",
    "    for idx, q in enumerate(questions, 1):\n",
    "        ans = find_answer(q, search_result_all)\n",
    "        result_list.append(f'{idx}. 问题：\"{q}\"\\n回答：\"\"\"{ans}\"\"\"\\n')\n",
    "    \n",
    "    result = ''.join(result_list)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_result_top_en(rank_search_result, search_result_all):\n",
    "    def extract_top_questions(text):\n",
    "        # Extract questions from ranked results\n",
    "        pattern = r'Question: (.*?)(?=\\n|$)'\n",
    "        matches = re.findall(pattern, text)\n",
    "        questions = [re.sub(r'\\*\\*', '', m).strip() for m in matches]\n",
    "        return questions[:3]\n",
    "    \n",
    "    def find_answer(question, all_text):\n",
    "        # Find matching answer for each question\n",
    "        escaped_q = re.escape(question)\n",
    "        pattern = f'Question [0-9]?: \"{escaped_q}\"\\\\nAnswer: \"\"\"(.*?)\"\"\"'\n",
    "        match = re.search(pattern, all_text, re.DOTALL)\n",
    "        return match.group(1).strip() if match else ''\n",
    "    \n",
    "    result_list = []\n",
    "    questions = extract_top_questions(rank_search_result)\n",
    "    \n",
    "    for idx, q in enumerate(questions, 1):\n",
    "        ans = find_answer(q, search_result_all)\n",
    "        result_list.append(f'{idx}. Question: \"{q}\"\\nAnswer: \"\"\"{ans}\"\"\"\\n')\n",
    "    \n",
    "    result = ''.join(result_list)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage III：Reasoning\n",
    "* Reasoning format\n",
    "* Explicit reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3_zh = '''\n",
    "图片描述：{}；\n",
    "关键词：{}；\n",
    "关键词描述：{}；\n",
    "请结合以上图片、图片关键词和描述信息，尽可能分析理解图文结合的深层含义。无需描述图文，仅回答图片隐喻。请保证回答的准确性并尽量简洁。\n",
    "'''\n",
    "\n",
    "prompt3_en = '''\n",
    "Image description: {}; \n",
    "Keywords: {}; \n",
    "Keywords description: {};\n",
    "Please combine the image, keywords, and description information and try to understand the deep meaning of the combination of the image and text. \\\n",
    "No need to describe images and text, only answer metaphors. Ensure the accuracy of the answer and try to be concise as much as possible.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt4_reasoning_zh = '''\n",
    "图片描述：{}；\n",
    "关键词：{}；\n",
    "关键词描述：{}；\n",
    "请结合以上图片、图片关键词和描述信息，尽可能分析理解图文结合的深层含义，回答以下单选题。直接回答正确选项 $LETTER。\\\n",
    "输出思考过程在 <think> </think> 标签中和最终正确答案在 <answer> </answer> 标签中。\\\n",
    "输出格式：<think>...</think> <answer>...</answer> \\\n",
    "\n",
    "单选题：{}\n",
    "答案：\n",
    "'''\n",
    "\n",
    "prompt4_reasoning_en = '''\n",
    "Image description: {}; \n",
    "Keywords: {}; \n",
    "Keywords description: {};\n",
    "Please combine the image, keywords, and description information and try to understand the deep meaning of the combination of the image and text. \\\n",
    "Answer the following multiple-choice questions with the correct option $LETTER directly. \\\n",
    "Output the thinking process in <think> </think> and final correct answer in <answer> </answer> tags. \\\n",
    "The output format should be as follows: <think>...</think> <answer>...</answer> \\\n",
    "\n",
    "Multiple-choice questions: {}\n",
    "Answer:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单选题\n",
    "prompt_MCQ_zh = '''\n",
    "问题：{}\n",
    "选项：{}\n",
    "'''\n",
    "\n",
    "prompt_MCQ_en = '''\n",
    "Question: {}\n",
    "Options: {}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 问答题\n",
    "prompt_OSQ = '''\n",
    "# Role\n",
    "You are an impartial judge who is familiar with Internet culture and memes, and is good at digging out and analyzing the deep meaning of Internet memes.\n",
    "\n",
    "## Attention\n",
    "You are responsible for evaluating the quality of the answer provided by the model for Internet culture and memes. Your evaluation should refer to the human answer and image, and score based on the Evaluation Standard.\n",
    "\n",
    "## Evaluation Standard:\n",
    "[1 point]: Fails to capture key elements within the image (such as text, and important entities). Does not identify emotions, domain, or rhetorical devices. Only provides a superficial description of surface-level information, lacking depth and creativity, with a significant gap from the standard answer.\n",
    "\n",
    "[2 points]: Captures some key elements within the image, but the identification of emotions, domain, and rhetorical devices is vague. The description of surface-level information is relatively complete, but there is a clear deficiency in exploring deeper meanings, showing a noticeable gap from the standard answer.\n",
    "\n",
    "[3 points]: Effectively captures key elements within the image and initially identifies emotions, domain, and rhetorical devices. The description of surface-level information is relatively accurate, and there is some relevant expression of deep meanings. However, there is still room for improvement in depth and creativity, and it is generally close to the standard answer.\n",
    "\n",
    "[4 points]: Accurately captures key elements within the image and clearly identifies emotions, domain, and rhetorical devices. The description of surface-level information is detailed and precise, with a relatively deep exploration of deep meanings, demonstrating a certain level of creativity and depth. It is largely consistent with the standard answer but may have minor deficiencies in some details or depth.\n",
    "\n",
    "[5 points]: Accurately and precisely captures key elements within the image and profoundly identifies emotions, domain, and rhetorical devices. The description of surface-level information is comprehensive and precise, with unique insights into deep meanings, skillfully integrating image elements with metaphorical implications. It demonstrates exceptional creativity and depth, is highly consistent with the standard answer, and shows a profound grasp of metaphor creation and cultural understanding.\n",
    "\n",
    "## Standrad Answer:\n",
    "Human answer: {}\n",
    "\n",
    "## Constraints\n",
    "- Avoid any position biases and be as objective as possible\n",
    "- Do not allow the length of the descriptions to influence your evaluation\n",
    "- Output your final directly by strictly following this format: \"[ratings]\"\n",
    "\n",
    "## Solve:\n",
    "Model answer: {}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成图片隐喻 p1 + p2 + p3\n",
    "def GPT_P3(url,prompt8,image_dep,keywords,overall_summary):    \n",
    "    base64_image = encode_image(url)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt8.format(image_dep, keywords, overall_summary)},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\", \"detail\": \"high\"}}\n",
    "        ]}\n",
    "    ]\n",
    "    api_key = \"\"\n",
    "    proxy_api_url = ''\n",
    "    headers = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {api_key}', }\n",
    "    data = {'model': 'gpt-4o-mini', 'messages': messages, 'temperature': 0.7, 'top_p': 0.9}\n",
    "    response = requests.post(proxy_api_url, headers=headers, json=data)\n",
    "    image_implication = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return image_implication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回答单选题 p1 + p2 + p3\n",
    "def GPT_P4(url,prompt9,image_dep,keywords,overall_summary,multiple_questions):    \n",
    "    base64_image = encode_image(url)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt9.format(image_dep, keywords, overall_summary, multiple_questions)},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\", \"detail\": \"high\"}}\n",
    "        ]}\n",
    "    ]\n",
    "    api_key = \"\"\n",
    "    proxy_api_url = ''\n",
    "    headers = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {api_key}', }\n",
    "    data = {'model': 'gpt-4o-mini', 'messages': messages, 'temperature': 0.5, 'top_p': 0.9}\n",
    "    response = requests.post(proxy_api_url, headers=headers, json=data)\n",
    "    MCQ_answer = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return MCQ_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_multiple_questions(prompt10, question_data):\n",
    "    question = question_data['question']\n",
    "    options = question_data['options']\n",
    "    multiple_options = ''\n",
    "    option_labels = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "    for label, option in zip(option_labels, options):\n",
    "        multiple_options += f'{label}. {option}\\n'\n",
    "    multiple_questions = prompt10.format(question, multiple_options)\n",
    "    return multiple_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回答问答题 p1 + p2 + p3\n",
    "def GPT_P5_EN(url,prompt11,explanation,image_implication):\n",
    "    base64_image = encode_image(url)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt11.format(explanation,image_implication)},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\", \"detail\": \"high\"}}\n",
    "        ]}\n",
    "    ]\n",
    "    api_key = \"\"\n",
    "    proxy_api_url = ''\n",
    "    headers = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {api_key}', }\n",
    "    data = {'model': 'gpt-4o-2024-11-20', 'messages': messages, 'temperature': 0, 'top_p': 0.9}\n",
    "\n",
    "    response = requests.post(proxy_api_url, headers=headers, json=data)\n",
    "    score = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return score\n",
    "\n",
    "def GPT_P5_ZH(url,prompt11,metaphorical_meaning,explanation,image_implication):\n",
    "    base64_image = encode_image(url)\n",
    "    human_answer = metaphorical_meaning + ';' + explanation\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": prompt11.format(human_answer,image_implication)},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\", \"detail\": \"high\"}}\n",
    "        ]}\n",
    "    ]\n",
    "    api_key = \"\"\n",
    "    proxy_api_url = ''\n",
    "    headers = { 'Content-Type': 'application/json', 'Authorization': f'Bearer {api_key}', }\n",
    "    data = {'model': 'gpt-4o-2024-11-20', 'messages': messages, 'temperature': 0, 'top_p': 0.9}\n",
    "\n",
    "    response = requests.post(proxy_api_url, headers=headers, json=data)\n",
    "    score = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "* 从数据集读入图片(中文和英文图片)\n",
    "* 生成图片描述(GPT_p1_1)\n",
    "* 生成图片关键词(GPT_p1_2)\n",
    "* 生成检索问题(GPT_p2_1) + 拆分检索问题(split_search_questions)\n",
    "* 检索问题 self-judge(GPT_p2_2) + 提取得分，选择search策略(choose_search_strategy)\n",
    "* ModelSearch(GPT_p2_3) / WebSearch(MindSearch) (choose_search_strategy)\n",
    "* 检索内容排序(GPT_p2_4)\n",
    "* 内容改写补充(GPT_p2_5)\n",
    "* 图片隐喻生成(GPT_p3)\n",
    "* 回答MCQ单选题(GPT_p4)\n",
    "* 回答OSQ问答题(GPT_p5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAD Pipeline (perception + search + reasoning)\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "with open('dataset/II-Bench.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 检查结果文件是否存在，如果存在则读取已有数据\n",
    "if os.path.exists('experiment/en/4o_mini_en_no_multiturn_ii.json'):\n",
    "    with open('experiment/en/4o_mini_en_no_multiturn_ii.json', 'r', encoding='utf-8') as f:\n",
    "        results = json.load(f)\n",
    "else:\n",
    "    results = []\n",
    "\n",
    "# 遍历所有数据并显示进度条\n",
    "for idx, item in enumerate(tqdm(data, desc=\"图片识别中\")):\n",
    "    try:\n",
    "        url = item['local_path']\n",
    "        # image_dep = GPT_p1_1(prompt1_1_zh, url)\n",
    "        image_dep = GPT_p1_1(prompt1_1_en, url)\n",
    "        print(\"image_dep: \", image_dep)\n",
    "        # keywords = GPT_p1_2(prompt1_2_zh, image_dep)\n",
    "        keywords = GPT_p1_2(prompt1_2_en, image_dep)\n",
    "        print(\"keywords: \", keywords)\n",
    "        print(\"\\n --------------------------------------------------- \\n\")\n",
    "\n",
    "        # search_questions_all = GPT_p2_1(prompt2_1_zh, keywords)\n",
    "        search_questions_all = GPT_p2_1(prompt2_1_en, keywords)\n",
    "        search_questions = split_search_questions(search_questions_all)\n",
    "        search_result_all = ''\n",
    "        for i, search_question in enumerate(search_questions):\n",
    "            # score = GPT_P2_2(prompt2_2_zh, search_question)\n",
    "            score = GPT_P2_2(prompt2_2_en, search_question)\n",
    "            score_value, decision = extract_score(score)\n",
    "            # search_result = choose_search_strategy(score_value, decision, prompt2_3_zh, search_question, i)\n",
    "            search_result = choose_search_strategy(score_value, decision, prompt2_3_en, search_question, i)\n",
    "            # search_result_single = f'问题{i}：{search_question}\\n回答：\"\"\"{search_result}\"\"\"\\n'\n",
    "            search_result_single = f'Question {i}: {search_question}\\nAnswer: \"\"\"{search_result}\"\"\"\\n'\n",
    "            print(\"search_result_single: \", search_result_single)\n",
    "            search_result_all += search_result_single\n",
    "\n",
    "        # rank_search_result = GPT_P2_4(prompt2_4_zh, image_dep, search_result_all)\n",
    "        rank_search_result = GPT_P2_4(prompt2_4_en, image_dep, search_result_all)\n",
    "        print(\"rank_search_result: \", rank_search_result)\n",
    "        \n",
    "        search_result_top = search_result_top_en(rank_search_result, search_result_all)\n",
    "        # search_result_top = search_result_top_zh(rank_search_result, search_result_all)\n",
    "        print(\"search_result_top: \", search_result_top)\n",
    "        \n",
    "        if search_result_top == '':\n",
    "            print(\"search_result_top is empty\")\n",
    "            search_result_top = search_result_all\n",
    "            \n",
    "        # overall_summary = GPT_P2_5(prompt2_5_zh, rank_search_result, search_result_top)\n",
    "        overall_summary = GPT_P2_5(prompt2_5_en, rank_search_result, search_result_top)\n",
    "        print(\"overall_summary: \", overall_summary)\n",
    "        # image_implication = GPT_P3(url, prompt3_zh, image_dep, keywords, overall_summary)\n",
    "        image_implication = GPT_P3(url, prompt3_en, image_dep, keywords, overall_summary)\n",
    "        print(\"image_implication: \", image_implication)\n",
    "        print(\"\\n --------------------------------------------------- \\n\")\n",
    "\n",
    "        question_data = item['questions'][0]\n",
    "        # multiple_questions = construct_multiple_questions(prompt_MCQ_zh, question_data)\n",
    "        multiple_questions = construct_multiple_questions(prompt_MCQ_en, question_data)\n",
    "        # MCQ_answer = GPT_P4(url, prompt4_reasoning_en, image_dep, keywords, overall_summary, multiple_questions)\n",
    "        MCQ_answer = GPT_P4(url, prompt4_reasoning_en, image_dep, keywords, overall_summary, multiple_questions)\n",
    "        print(\"MCQ_answer: \", MCQ_answer)\n",
    "        print(\"\\n --------------------------------------------------- \\n\")\n",
    "\n",
    "        explanation = item['meta_data']['explanation']\n",
    "        # metaphorical_meaning = item['meta_data']['metaphorical_meaning']\n",
    "        # print(metaphorical_meaning + ';' + explanation)\n",
    "        # OSQ_answer = GPT_P5_ZH(url, prompt_OSQ, metaphorical_meaning, explanation, image_implication)\n",
    "        OSQ_answer = GPT_P5_EN(url, prompt_OSQ, explanation, image_implication)\n",
    "        print(\"OSQ_answer: \", OSQ_answer)\n",
    "        print(\"\\n --------------------------------------------------- \\n\")\n",
    "\n",
    "        # 组织结果\n",
    "        result = {\n",
    "            'id': idx,\n",
    "            'url': url,\n",
    "            'image_dep': image_dep,\n",
    "            'keywords': keywords,\n",
    "            'search_questions': search_questions,\n",
    "            'search_result_all': search_result_all,\n",
    "            'rank_search_result': rank_search_result,\n",
    "            'search_result_top': search_result_top,\n",
    "            'overall_summary': overall_summary,\n",
    "            'image_implication': image_implication,\n",
    "            'multiple_questions': multiple_questions,\n",
    "            'MCQ_answer': MCQ_answer,\n",
    "            'OSQ_answer': OSQ_answer,\n",
    "        }\n",
    "\n",
    "        # 将结果添加到列表\n",
    "        results.append(result)\n",
    "\n",
    "        # 将更新后的结果写入 JSON 文件\n",
    "        with open('experiment/en/4o_mini_en_no_multiturn_ii.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    except Exception as e:\n",
    "        # 处理异常情况\n",
    "        print(f'处理图片 {url} 时出错：{e}')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCQ\n",
    "import json\n",
    "\n",
    "with open('dataset/II-Bench.json', 'r', encoding='utf-8') as f:\n",
    "    cii_data = json.load(f)\n",
    "\n",
    "with open('experiment/en/4o_mini_en_no_multiturn_ii.json', 'r', encoding='utf-8') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "# 建立url和对应的正确答案的映射\n",
    "test_dict = {}\n",
    "for item in test:\n",
    "    url = item['url']\n",
    "    pattern = r'<answer>\\s*([A-Za-z])' # reasoning format\n",
    "    match = re.search(pattern, item['MCQ_answer'])\n",
    "    if match:\n",
    "        MCQ_answer = match.group(1)\n",
    "    else:\n",
    "        MCQ_answer = None\n",
    "    test_dict[url] = MCQ_answer\n",
    "\n",
    "# 比较答案并计算正确率\n",
    "total_questions = 0\n",
    "correct_count = 0\n",
    "result_lines = []\n",
    "\n",
    "for item in cii_data:\n",
    "    local_path = item['local_path']\n",
    "    questions = item['questions']\n",
    "    for question in questions:\n",
    "        total_questions += 1\n",
    "        answer = question['answer'].strip()\n",
    "        question_id = question['id']\n",
    "        if local_path in test_dict:\n",
    "            MCQ_answer = test_dict[local_path]\n",
    "            if answer == MCQ_answer:\n",
    "                correct_count += 1\n",
    "                result = f\"图片：{local_path}，题目ID：{question_id}，答案一致：{answer}\"\n",
    "            else:\n",
    "                result = f\"图片：{local_path}，题目ID：{question_id}，答案不一致，标准答案：{answer}，测试答案：{MCQ_answer}\"\n",
    "        else:\n",
    "            result = f\"图片：{local_path}在文件中未找到对应条目。\"\n",
    "            print(f\"报错：{question_id}未找到\")\n",
    "            total_questions -= 1\n",
    "        result_lines.append(result)\n",
    "\n",
    "accuracy = correct_count / total_questions * 100\n",
    "result_lines.append(f\"\\n总题目数：{total_questions}，正确数量：{correct_count}，正确率：{accuracy:.2f}%\")\n",
    "print(f\"总题目数：{total_questions}，正确数量：{correct_count}，正确率：{accuracy:.2f}%\")\n",
    "\n",
    "# 将结果保存到answer.txt文件中\n",
    "with open('results/en/MCQ/answer_4o_mini_en_no_multiturn_ii.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in result_lines:\n",
    "        f.write(line + '\\n')\n",
    "    print(\"结果已保存\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OSQ\n",
    "import re\n",
    "import json\n",
    "\n",
    "# 读取JSON文件\n",
    "with open('experiment/en/4o_mini_en_no_multiturn_ii.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "count = 0\n",
    "total_score = 0\n",
    "\n",
    "# 遍历每个实例，累加分数并计数\n",
    "for instance in data:\n",
    "    count = count + 1\n",
    "    score_str = instance.get('OSQ_answer', '[]')\n",
    "    score_match = re.search(r'\\d+', score_str)\n",
    "    score = int(score_match.group()) if score_match else 0\n",
    "    total_score += score\n",
    "\n",
    "# 计算总体平均分数\n",
    "average_score = total_score / count\n",
    "print(f'总体平均分数: {average_score:.2f}, 共{count}个实例')\n",
    "\n",
    "summary = f\"总体平均分数: {average_score:.2f}, 共{count}个实例\"\n",
    "\n",
    "# 将结果保存到answer.txt文件中\n",
    "with open('results/en/OSQ/answer_4o_mini_en_no_multiturn_ii.txt', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=4)\n",
    "    print(\"结果已保存\")   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
